# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ylll8j95fXmmeYN4p-nysGHeXWwe0wiv
"""



from textblob import TextBlob
import pandas as pd
from pandas.core import describe
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

reviews = pd.read_csv('/content/reviews.csv')
reviews = reviews.dropna(subset=['comments'])
listings = pd.read_csv('/content/listings.csv')
listings = listings.dropna(subset=['neighbourhood_cleansed'])

# Remove non-numeric values in the 'id' column in listings
listings['id'] = pd.to_numeric(listings['id'], errors='coerce')
listings = listings.dropna(subset=['id'])

# Remove non-numeric values in the 'listing_id' column in reviews
reviews['listing_id'] = pd.to_numeric(reviews['listing_id'], errors='coerce')
reviews = reviews.dropna(subset=['listing_id'])

# Drop the 'reviewer' column in reviews
reviews = reviews.drop(columns=['reviewer_name'])

# Filter listings and reviews for data from 2018 onwards
reviews['date'] = pd.to_datetime(reviews['date'], errors='coerce')
reviews = reviews[reviews['date'] >= '2018-01-01']

stop_words = set(stopwords.words('english'))
wordnet_lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]
    return filtered_tokens

reviews['processed_comments'] = reviews['comments'].apply(preprocess_text)

# Perform sentiment analysis using TextBlob
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

reviews['sentiment'] = reviews['comments'].apply(get_sentiment)

# Display the first few rows of the reviews DataFrame with processed comments and sentiment scores
print(reviews.head())

# Merge the 'neighbourhood_cleansed' column from the listings DataFrame to the reviews DataFrame
reviews = reviews.merge(listings[['id', 'neighbourhood_cleansed']], left_on='listing_id', right_on='id', how='left')

# Display the first few rows of the updated reviews DataFrame
print(reviews.head())

# Drop the extra 'id' column
reviews = reviews.drop(columns=['id_y'])
print(reviews.head())

# Group the reviews by neighbourhood and calculate the mean sentiment for each neighbourhood
average_sentiment_by_neighbourhood = reviews.groupby('neighbourhood_cleansed')['sentiment'].mean().reset_index()

# Rename the columns for clarity
average_sentiment_by_neighbourhood.columns = ['neighbourhood_cleansed', 'average_neighbourhood_sentiment']

# Merge the average_neighbourhood_sentiment column with the reviews DataFrame
reviews = reviews.merge(average_sentiment_by_neighbourhood, on='neighbourhood_cleansed', how='left')

# Display the first 5 rows of the updated DataFrame
reviews.head()

reviews = reviews.drop(columns=['comments', 'processed_comments'])
print(reviews.head())

reviews.to_csv('sentiment_score.csv', index=False)